This XML file does not appear to have any style information associated with it. The document tree is shown below.
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html" version="2.0">
<channel>
<title>
<![CDATA[ Stories by Cengiz Han on Medium ]]>
</title>
<description>
<![CDATA[ Stories by Cengiz Han on Medium ]]>
</description>
<link>https://medium.com/@hancengiz?source=rss-7a7fc7009fee------2</link>
<image>
<url>https://cdn-images-1.medium.com/fit/c/150/150/1*LPc-x3bDKtveUreIFA8OVw.jpeg</url>
<title>Stories by Cengiz Han on Medium</title>
<link>https://medium.com/@hancengiz?source=rss-7a7fc7009fee------2</link>
</image>
<generator>Medium</generator>
<lastBuildDate>Tue, 21 Oct 2025 18:24:04 GMT</lastBuildDate>
<atom:link href="https://medium.com/@hancengiz/feed" rel="self" type="application/rss+xml"/>
<webMaster>
<![CDATA[ yourfriends@medium.com ]]>
</webMaster>
<atom:link href="http://medium.superfeedr.com" rel="hub"/>
<item>
<title>
<![CDATA[ Beyond Vibe-Coding: Spec-Driven Development ]]>
</title>
<link>https://medium.com/@hancengiz/beyond-vibe-coding-spec-driven-development-80e80aade50e?source=rss-7a7fc7009fee------2</link>
<guid isPermaLink="false">https://medium.com/p/80e80aade50e</guid>
<category>
<![CDATA[ kiro ]]>
</category>
<category>
<![CDATA[ specs ]]>
</category>
<category>
<![CDATA[ claude-code ]]>
</category>
<category>
<![CDATA[ ai-native ]]>
</category>
<category>
<![CDATA[ ai-native-engineering ]]>
</category>
<dc:creator>
<![CDATA[ Cengiz Han ]]>
</dc:creator>
<pubDate>Mon, 28 Jul 2025 12:31:48 GMT</pubDate>
<atom:updated>2025-07-28T13:17:39.084Z</atom:updated>
<content:encoded>
<![CDATA[ <p>Vibe-coding is fun. You throw an idea at the AI, see what it spits out, tweak it, and repeat. For side projects, that’s fine.</p><p>But for production systems? Real products? <strong>Enterprises don’t ship on vibes.</strong></p><h3>We’re Coding in English Now. So What?</h3><p>We’re heading into a world where English <em>is</em> the new interface. I’ve never had to write a line of assembly code. Most devs today never see bytecode. Pretty soon, many won’t touch a traditional language either.</p><p>That doesn’t make precision any less critical. It simply means it needs to be specified earlier in the specs.</p><p>You’re not writing for the compiler anymore. You’re crafting a prompt for the AI. If your prompt is unclear, the result will not only be buggy but also incorrect in ways you&#39;ll only realize too late.</p><p>Like I said in a recent tweet:</p><iframe src="https://cdn.embedly.com/widgets/media.html?type=text%2Fhtml&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;schema=twitter&amp;url=https%3A//x.com/hancengiz/status/1945469135829299393&amp;image=" width="500" height="281" frameborder="0" scrolling="no"><a href="https://medium.com/media/125dfd26aaa8ea5446634f8986622f4f/href">https://medium.com/media/125dfd26aaa8ea5446634f8986622f4f/href</a></iframe><p>That’s the whole point. The interface might change, but the need for clarity doesn’t.</p><h3>Spec-Driven Development: Less Magic, More Alignment</h3><p>Instead of jumping straight into prompts or code, I start with a spec. Simple as that.</p><ul><li>What should the system do?</li><li>What’s out of scope?</li><li>What happens when something breaks?</li><li>What are the business rules?</li></ul><p>This doesn’t mean writing 30 pages of documentation before every sprint. A good spec might be a short markdown file. But it’s clear. It’s testable. It provides the AI (or another developer) with something to align to.</p><p>And when that spec becomes the source of truth, everything flows better: code, tests, documentation, and even conversations.</p><h3>Amazon Kiro Is a Glimpse of What’s Coming</h3><p>You can already see the direction this is heading. Amazon recently launched <a href="https://kiro.dev/docs/specs/concepts/">Kiro</a>, an AI agent designed to assist in creating workflows and infrastructure But it doesn’t just ask you to describe your app. It starts with a structured spec.</p><p>Why? Because specs reduce ambiguity. They make the AI’s job easier. And they make your code more predictable. That design choice says a lot.</p><p>This spec-first mindset isn’t a trend. It’s a design pattern for tools that want to build things that actually work.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XtjLorayncWXZOlp-usviQ.png" /></figure><h3>AI Makes Things Faster. Specs Make Them Safer.</h3><p>Yes, AI can speed things up. No doubt about that. But speed without structure is a mess waiting to happen. It’s seen it play out in teams that moved fast, skipped the alignment, and spent months cleaning up avoidable bugs.</p><p>Specs don’t slow you down. They stop you from crashing later.</p><p>Having your system ready before starting to write code or prompts helps everyone work more efficiently and reduces unexpected issues.</p><h3>TL;DR</h3><ul><li>Coding in English is here, but clarity still matters.</li><li>Prompting without structure leads to drift, bugs, and fragile systems.</li><li>Spec-first thinking ensures everything remains aligned, whether your team consists of humans or AI.</li><li>Tools like Kiro show where things are headed: structured input, reliable output.</li><li>Vibes are fun. Specs are how you ship.</li></ul><p>Write the spec. Let the tools do the rest. That’s how production gets done.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=80e80aade50e" width="1" height="1" alt=""> ]]>
</content:encoded>
</item>
<item>
<title>
<![CDATA[ (Micro)services with Event Notifications ]]>
</title>
<link>https://medium.com/hepsiburadatech/micro-services-with-event-notifications-c8462792e700?source=rss-7a7fc7009fee------2</link>
<guid isPermaLink="false">https://medium.com/p/c8462792e700</guid>
<category>
<![CDATA[ microservices ]]>
</category>
<dc:creator>
<![CDATA[ Cengiz Han ]]>
</dc:creator>
<pubDate>Mon, 25 Feb 2019 09:25:56 GMT</pubDate>
<atom:updated>2019-02-25T09:25:56.584Z</atom:updated>
<content:encoded>
<![CDATA[ <p>Microservices is an architectural style that describes software design as independently deployable, loosely coupled services which are modelled around particular business domain.</p><p>Extracting different business domain capabilities from a single process monolith application and creating a system design that contains smaller service processes enables you to <em>scale</em> and <em>deploy</em> each service separately.</p><p>The fact that each service needs to be deployable separately requires and also enables application deployment automation, Continuous Delivery.</p><p>Martin Fowler describes pre requirements of microservices in his articles and emphasises the importance of <a href="https://martinfowler.com/bliki/DevOpsCulture.html">DevOpsCulture</a>.</p><blockquote><a href="https://martinfowler.com/bliki/MicroservicePrerequisites.html">You must be this tall to use microservices</a>.</blockquote><p>I have been mostly working with micro-services and event-driven systems for almost last 7 years. In this post, I will try to explain my view on a particular challenge when designing your service oriented system.</p><p><strong>How they talk to each other?</strong></p><p>One of the fastest way people start with is service to service communication between services. It is a fast way to start making call from to another but it brings lots of problems with it. Going down this route creates a distributed mess and brings problems of all services being available all the time and each services possibly depending on each other which makes things like failing safely really hard, mostly impossible.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*eNEo7vpTnzEQtxuse3fWyw.png" /><figcaption>Microservices directly calling each other. A bit of mess right?</figcaption></figure><p>You always need to plan for failures and how are you going to handle them, how are you going to <strong>fail safely</strong>. Service to service sync communication makes handling failures way much harder. When a business operation needs orchestration of a couple services to be successfully completed you need to <strong><em>start thinking about decoupling services, eventual consistency and bounded context</em></strong> (a way of defining boundaries of a complex domain into business context).</p><blockquote>A system’s being “<strong>fail</strong>-<strong>safe</strong>” means not that <strong>failure</strong> is impossible or improbable, but rather that the system’s design prevents or mitigates unsafe consequences of the system’s <strong>failure</strong>. That is, if and when a “<strong>fail</strong>-<strong>safe</strong>” system “<strong>fails</strong>”, it is “<strong>safe</strong>” or at least no less <strong>safe</strong> than when it was operating correctly. <a href="https://en.wikipedia.org/wiki/Fail-safe">wikipedia</a>.</blockquote><p>For example, what happens if SMTP service is down or you get an error message back from Notification Service when you call it from Order Service? Are you going to cancel the order for that reason? Of course not! If you are doing service to service communication, you need to create a retry mechanism in sync context or persist a state somewhere that needs to be processed by a background running job to keep sending notifications later when notification service is back online and functioning. And you need to think about all the failure scenarios in all service to service communications, no need to say it is a complex solution, <strong><em>guaranteeing reliability and maintainability, and keeping operability</em> <em>simple</em></strong> becomes a hard thing to accomplish.</p><h3>Event Driven Architecture</h3><p>Event Driven Architecture is mostly known as storing all the application state changes as sequence of events. But when you look at the different implementations on different projects we see different usage patterns under the name of Event Sourcing, Command Query Responsibility Segregation (CQRS), Event Notification. Martin Fowler published an article and talked about different type of Event Driven Architecture’s on <em>April, 2017 on a GoTo Conference </em>and created a better classification for different models we were using under the name of Event Driven Architecture. It is not my place to classify them when a guru has done it already.</p><p>I have used CQRS before in two projects, one of them went production, one of them was changed to <strong><em>“Events as secondary concern” </em></strong>as I called back then without knowing the better name to identify the pattern. I now know, it is named as <strong><em>Event Notification.</em></strong></p><blockquote><strong><em>Event notification</em></strong><em>:</em> components communicating via events<br><strong><em>Event-based State Transfer</em></strong><em>:</em> allowing components to access data without calling the source.<br><strong><em>Event Sourcing</em></strong>: using an event log as the primary record for a system<br><strong><em>CQRS</em></strong><em>:</em> having a separate component for updating a store from any readers of the store</blockquote><blockquote>Please see <a href="https://martinfowler.com/articles/201701-event-driven.html">Martin’s post</a> and video on this link for further information about different types.</blockquote><h3>Event Notification</h3><p>There is a slight difference between Event Notification and Event-base State Transfer. In <em>EbST</em> design events contain all the information for consumer service, but Event Notification just contains information about event and consumer services needs to go and get the all context about that event from origin service. Imagine you have an event called OrderCreated and that event contains OrderNumber and maybe some other metadata about order, but if you are creating a Notification Service to send an email/sms to customer about their recent order you might need to go to Order Service and ask details of that order with order number you just received via OrderCreated event.</p><p>Event Notification provides great decoupling and it allows other systems to hook up to events without telling it to source event. You can create a new service which is interested with an event from any service without telling or asking any change to the originating service.</p><p>One gotcha about this design you need to find a way to get all dependencies and which service depends what event from which service. You can not tell what happens in all system when a particular event happens, you can not just read code in source service and see what happens after that event. You need to go through all subscriptions and find out what happens in whole system. This is generally the case we all ignore till we end up in a place we have no idea what happens in the system.</p><p>One of the further things to think about once you started using Event Notification is what goes in to the events? Should you use Event-based State Transfer or do you need to call Order Service to get more information about order that was just created.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*8mj8vDAzgPEfs7lzzgRFAg.png" /><figcaption>Microservices does not know each other, they are decoupled by event notification</figcaption></figure><p>In the above model, when you use Event Notification model you create services that publish events and downstream services subscribes to those events on your event stream. This way, you push the complexity of handling failures in to event stream and event stream processors, your downstream services.</p><p>Order Service publishes an order created event and notification service subscribes to OrderCreated event and send a notification to customer.</p><p>Event store system stores all your events, you can use something like Kafka, RabbitMQ, nsq to store events and dispatch each event type to subscribers. If a service is down temporarily event store sends it to subscriber when they are back, you do not need to implement anything different in this scenario your system design handles it automatically.</p><p>Now you have an event driven architecture, that <em>notifies</em> other system when something happens, when you have a new requirement to react to O<em>rderCreated</em> event you do not need to go and make any changes in your Order Service. All you need to do is create a new subscriber service that subscribes to O<em>rderCreated</em> event.</p><p>Say, you want to show your customers product reviews separately by other customers who actually bought that product. They are verified customer reviews and more valuable to potential buyers, so you create a new subscription in your Customer Product Reviews Service to listen for O<em>rderCreated</em> and other order related events to mark customers as verified customers for that product. Very simple, it does not touch the existing part of the system, you build and deploy it separately without touching upstream service.</p><p>I am planning to write a follow up post with a code example to demonstrate what I mentioned here. Till then I highly recommend to watch <a href="https://martinfowler.com/articles/201701-event-driven.html">Martin’s keynote on goto conference</a>.</p><p><em>Originally published at </em><a href="https://medium.com/p/4425ef5be871"><em>cengizhan.com</em></a><em> on September 4, 2017.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c8462792e700" width="1" height="1" alt=""><hr><p><a href="https://medium.com/hepsiburadatech/micro-services-with-event-notifications-c8462792e700">(Micro)services with Event Notifications</a> was originally published in <a href="https://medium.com/hepsiburadatech">hepsiburadatech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]>
</content:encoded>
</item>
<item>
<title>
<![CDATA[ 3 Pillars of Observability ]]>
</title>
<link>https://medium.com/hepsiburadatech/3-pillars-of-observability-d458c765dd26?source=rss-7a7fc7009fee------2</link>
<guid isPermaLink="false">https://medium.com/p/d458c765dd26</guid>
<category>
<![CDATA[ devops ]]>
</category>
<dc:creator>
<![CDATA[ Cengiz Han ]]>
</dc:creator>
<pubDate>Mon, 25 Feb 2019 09:24:06 GMT</pubDate>
<atom:updated>2019-05-08T06:53:05.788Z</atom:updated>
<content:encoded>
<![CDATA[ <p>Observability of the system in production comes as a requirement when we design complex systems. Some says being able to monitor your system in production is more important than testing all of it’s functionality during development. To me, they are not really comparable things or you can give up one of another.</p><p>Traditionally, if you have IT operations department in your organization you probably have people who does <strong>blackbox monitoring</strong> with tools like Nagios. What this tools give you are signals like <em>system is down, server/service is down, CPU consumption high etc. </em>This is a must have and very good for identifying the <em>symptoms</em> of a problem but not the <em>root cause</em>.</p><p>Once you get this symptoms telling you there is something wrong. You need to dive deep and understand the root cause. <strong>Whitebox monitoring</strong> comes in to the picture here. Whitebox monitoring can help you to identify root cause of a problem and also more importantly can give you proactive alerting for the possible preventable problems by looking at some tendencies on the system if it is designed right. Because internals of an application can provide more valuable and actionable alerts to take actions on critical cases or notice things like performance problems to be more proactive and take actions before things go down.</p><p>Logging, metrics and distributed tracing on the other hand are whitebox monitoring that refers to a category of monitoring tools and techniques that work with data reported from the internals of a system. I would like to write about these 3 pillars of observability in the scope of whitebox monitoring. When position these tools correctly you might not need to do blackbox monitoring that often, but still good to keep them on if you ask me.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/400/1*qhHpwk7Z2SQrVGpfXZgbDg.png" /></figure><ul><li><strong><em>Logging</em></strong></li><li><strong><em>Metrics</em></strong></li><li><strong><em>Distributed Tracing</em></strong></li></ul><p>What are the differences of this three and how we accomplish this foundation with these 3 pillars.</p><h3>Logging</h3><p>This is something probably the most systems I ever worked has implemented.</p><p>Logs are events happening in your system, these are detailed, prioritized messages from your system. I think thinking of logs are events in your system is not false idea.</p><p>The biggest drawbacks of logs is the expensive to process, store and ship. They contain data for every single request that happened to your system. If you are running your application on hundreds of servers you need to aggregate them carefully to a central location otherwise it becomes impossible to check them on each server.ELK is the most common stack here as you probably know.</p><p>By saying that, there are also some drawback of shipping all to logs to be aggregated centrally. If you are dealing with huge volume of traffic and you might need to think of what to ship, what not to ship (hint: correct logging levels) also you need to have the right scale for your aggregation clusters, in most cases elasticsearch cluster. It is not uncommon to have a cluster of elasticsearch to aggregate all the logs and it fails to catchup when there is a spike of logs on the days like Black Friday.</p><p>Libraries like SLF4J, log4j, log4net (there are lots of options depending on the tech stack you are on) are being used to create formatted plaintext logs. Most popular way of shipping your application logs is writing them to files on the disk and shipping them to ELK with tools like FileBeat. But your application can also ship your logs directly to your log aggregator. There are lots of options you can evaluate for your case. Once I developed a log4net appender which pushes logs as messages to amqp (we were using rabbitmq for this) then we were using logstash to receive logs from rabbitmq and insert them to elasticsearch then visualize with Kibana.</p><p>Recently we started to use Docker Engine to ship our logs. Docker added a feature to ship logs to central log repositories like ELK stack. Most of central logging repositories I know support Graylog Extended Log Format (GELF) and I believe that is <a href="https://docs.docker.com/engine/admin/logging/overview/#configure-the-default-logging-driver">how docker engine ships.</a></p><p>You can also get logs from your infrastructure tools. Most of the popular message brokers (things like kafka, rabbitmq, nsq), HTTP reverse proxies, load balancers, databases, firewalls, application servers, middlewares provide their logs and you can ship them to your central log aggregators.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gtMm0CIE3eNgr4RbSKruDQ.png" /></figure><h3>Metrics</h3><p>Metrics are numbers that aggregatable and measured over intervals of time as time series data. Metrics are optimized for storage, processing of the data since they are just numbers aggregated over intervals of time.</p><p>One of the advantages of metrics based monitoring is overhead of metrics generation and storage is constant, it does not change like logs based monitoring in direct proportion to the increase of the system load. That means Disk or processing utilization does not change based on the increase of traffic. Disk storage utilization only increase based on data on time series database being captured, which happens when you add new metrics’ in your instrumentation in your application code or when you spin up new services/containers/hosts.</p><p>Prometheus (p8s) clients does not send each and every metric to the p8s. Popular prometheus client libraries, for example Coda Hale’s popular <a href="http://metrics.dropwizard.io/">metrics</a> library (and it is java but this project direct ports to different languages) aggregates time series data in your application process and generates metrics output based on in-process calculations. I recommend watching <a href="https://www.youtube.com/watch?v=czes-oa0yik">his presentation on youtube</a> if you want to learn more about his metrics library.</p><p>So, you need to add instrumentation to your application code first if you want to start using Prometheus to collect metrics from your application. You can find a list of <a href="https://prometheus.io/docs/instrumenting/clientlibs/">client libraries</a> on p8s web site. Prometheus works pull based basically you use one of the available to collect metrics in your application and then expose them on your application as accessible via HTTP, generally /metrics endpoint in your application. Then you go an configure prometheus to scrape metrics from your application every few seconds.</p><p>Metrics are far more efficient than querying and aggregating log data. But logs can give you exact data, if you want to get exact average of response times of your server you can log them and then write aggregation queries on elasticsearch. We have to member that metrics are not hundred percent accurate, they relay on some statistical algorithm. Tools like Prometheus and popular metrics client libraries implements some advanced algorithms to give us most accurate numbers. Do not get me wrong! I am not saying use logs, I am saying use both, logs and metrics for the right purpose.</p><p>Finally if you are want to learn Prometheus from scratch and if you like learning it from vides like me, I highly recommend this talk: <a href="https://www.youtube.com/watch?v=5GYe_-qqP30">Infrastructure and application monitoring using Prometheus by Marco Pas</a></p><p>Once you collect all your metrics in Prometheus, you can use <a href="http://www.grafana.net">Grafana</a> to visualize those metrics.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hUaJjsAgi0zDH4vuTuL_NA.png" /><figcaption>Metrics on Prometheus</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5314_r4XulcKDKsniFgT1g.png" /><figcaption>Prometheus data visualized on Grafana</figcaption></figure><p><strong>What should I collect?</strong></p><p>Once you have the setup to collect your own this is the question you need to answer. If you are adding metrics for a microservice;</p><p>Firstly, I would recommend you to start with capturing number of requests to observe how busy your service is and how many requests you receive per second/minute. <strong>Number of Requests</strong></p><p>Secondly, I would say start capturing your service’s service time. Basically duration of each request to capture what is your service’s latency. <strong>Service Response Time</strong></p><p>And, I would say capture number of erroneous requests to observe of what percentage of the requests coming to your service are failing. <strong>Error rate of requests.</strong></p><p>Lastly, <strong>always check %95 percentile</strong> if you are not sure what percentile to check. Mean time or average is a happy picture if you want to trick yourself.</p><p>There will be very specific cases for your application, these are just some suggestions you can start to think of. For example in our last project we wanted to measure ETL processing time of each product. We captured each product’s update rate in the underlying system and we calculated the time it took to get to the end of the ETL pipeline. So this way we wanted to see if there is a bottleneck in the Kafka based data streaming pipeline. This way we could observe each stage of data streaming pipeline to identify bottleneck and provision new Kafka Streams containers or Kafka Connect containers when needed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DABlYEqThloWwFpjjJusyg.png" /><figcaption>Monitoring product update latency on our Kafka data streaming pipeline.</figcaption></figure><p>Logs and metrics both needs to exists in your application monitoring stack and they need to be owned by the your team to build, not an IT Ops team. Logs can give you insight about each single request and look for details of what happened exactly at a specific time but metrics can show you context and understand trends in our system.</p><h3>Distributed Tracing</h3><p>When logs can give you insight about a specific time and see what happened it is hard to correlate them when you are building a distributed system. Especially in the era of microservices, a request from a customer can cause hundreds of different service calls in your application.</p><p>Monitoring calls that took longer than expected, call that are failed, why they are failed can be hard to do with logs. Also finding matching logs with a unique request id is something you can accomplish but it would still be hard to query slowest calls that my customers faced.</p><p>Google published a paper with the title <a href="https://research.google.com/pubs/pub36356.html"><strong>Dapper</strong>, a Large-Scale Distributed Systems Tracing Infrastructure</a> at 2010. They talked about how they trace distributed calls. June 2012 Twitter open sourced their internal distributed tracing project, <strong>Zipkin</strong>.</p><p>So if you are in the world of microservices and working on distributed system, you can imagine how valuable to have a visual of correlated distributed calls between services. I tried Zipkin in it’s early years and it was not easy to setup but now in the era of container, it is just one single command. But everyone was not using it and still not using probably. <a href="http://opentracing.io/">OpenTracing</a> was introduced as one single standard as all OSS projects and your application code to instrument your code without depending on one particular tracing vendor.</p><p>So now, you can use one of the <a href="http://opentracing.io/">listed</a> open source client libraries to instrument your code and publish this span information to one of the supported <a href="http://opentracing.io/documentation/pages/supported-tracers.html">tracers</a> (<a href="http://zipkin.io/">Zipkin</a>, <a href="http://uber.github.io/jaeger">Jaeger</a>, Appdash, LightStep, Hawkular, Instane and more).</p><p>If you remember your browsers developer tools and check network tab which calls are being made, it gives you very good insight about what your browsers does, which calls are made in parallell, which ones are taking too long to process and makes your customer wait. Distributed tracers gives you this kind of visualization, on server side.</p><p>For example, you can see which of your services being called, which ones take longer than expected or which ones fail when you receive a request to get list of products under a specific category, order by prices.</p><p>Zipkin interface let’s you query by longest and shortest duration of call stack. So you can focus on your low performing calls and understand which part of the system being a bottleneck. You can also get visualization of dependencies between services which becomes very valuable when you have hundreds of systems talking to each other.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OW_udTV1VSOYflz8kU_q7A.png" /><figcaption>A detailed view of a Zipkin trace</figcaption></figure><p><em>Originally published at </em><a href="https://cengizhan.com/3-pillars-of-observability-8e6cb5434206"><em>cengizhan.com</em></a><em> on November 19, 2017.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d458c765dd26" width="1" height="1" alt=""><hr><p><a href="https://medium.com/hepsiburadatech/3-pillars-of-observability-d458c765dd26">3 Pillars of Observability</a> was originally published in <a href="https://medium.com/hepsiburadatech">hepsiburadatech</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]>
</content:encoded>
</item>
<item>
<title>
<![CDATA[ 3 Pillars of Observability ]]>
</title>
<link>https://medium.com/hancengiz/3-pillars-of-observability-8e6cb5434206?source=rss-7a7fc7009fee------2</link>
<guid isPermaLink="false">https://medium.com/p/8e6cb5434206</guid>
<category>
<![CDATA[ observability ]]>
</category>
<category>
<![CDATA[ scaling-microservices ]]>
</category>
<category>
<![CDATA[ devops ]]>
</category>
<category>
<![CDATA[ microservices ]]>
</category>
<category>
<![CDATA[ monitoring ]]>
</category>
<dc:creator>
<![CDATA[ Cengiz Han ]]>
</dc:creator>
<pubDate>Sun, 19 Nov 2017 17:54:47 GMT</pubDate>
<atom:updated>2019-05-08T06:52:16.157Z</atom:updated>
<content:encoded>
<![CDATA[ <p>Observability of the system in production comes as a requirement when we design complex systems. Some says being able to monitor your system in production is more important than testing all of it’s functionality during development. To me, they are not really comparable things or you can give up one of another.</p><p>Traditionally, if you have IT operations department in your organization you probably have people who does <strong>blackbox monitoring</strong> with tools like Nagios. What this tools give you are signals like <em>system is down, server/service is down, CPU consumption high etc. </em>This is a must have and very good for identifying the <em>symptoms</em> of a problem but not the <em>root cause</em>.</p><p>Once you get this symptoms telling you there is something wrong. You need to dive deep and understand the root cause. <strong>Whitebox monitoring</strong> comes in to the picture here. Whitebox monitoring can help you to identify root cause of a problem and also more importantly can give you proactive alerting for the possible preventable problems by looking at some tendencies on the system if it is designed right. Because internals of an application can provide more valuable and actionable alerts to take actions on critical cases or notice things like performance problems to be more proactive and take actions before things go down.</p><p>Logging, metrics and distributed tracing on the other hand are whitebox monitoring that refers to a category of monitoring tools and techniques that work with data reported from the internals of a system. I would like to write about these 3 pillars of observability in the scope of whitebox monitoring. When position these tools correctly you might not need to do blackbox monitoring that often, but still good to keep them on if you ask me.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/400/1*qhHpwk7Z2SQrVGpfXZgbDg.png" /></figure><ul><li><strong><em>Logging</em></strong></li><li><strong><em>Metrics</em></strong></li><li><strong><em>Distributed Tracing</em></strong></li></ul><p>What are the differences of this three and how we accomplish this foundation with these 3 pillars.</p><h3><strong>Logging</strong></h3><p>This is something probably the most systems I ever worked has implemented.</p><p>Logs are events happening in your system, these are detailed, prioritized messages from your system. I think thinking of logs are events in your system is not false idea.</p><p>The biggest drawbacks of logs is the expensive to process, store and ship. They contain data for every single request that happened to your system. If you are running your application on hundreds of servers you need to aggregate them carefully to a central location otherwise it becomes impossible to check them on each server.ELK is the most common stack here as you probably know.</p><p>By saying that, there are also some drawback of shipping all to logs to be aggregated centrally. If you are dealing with huge volume of traffic and you might need to think of what to ship, what not to ship (hint: correct logging levels) also you need to have the right scale for your aggregation clusters, in most cases elasticsearch cluster. It is not uncommon to have a cluster of elasticsearch to aggregate all the logs and it fails to catchup when there is a spike of logs on the days like Black Friday.</p><p>Libraries like SLF4J, log4j, log4net (there are lots of options depending on the tech stack you are on) are being used to create formatted plaintext logs. Most popular way of shipping your application logs is writing them to files on the disk and shipping them to ELK with tools like FileBeat. But your application can also ship your logs directly to your log aggregator. There are lots of options you can evaluate for your case. Once I developed a log4net appender which pushes logs as messages to amqp (we were using rabbitmq for this) then we were using logstash to receive logs from rabbitmq and insert them to elasticsearch then visualize with Kibana.</p><p>Recently we started to use Docker Engine to ship our logs. Docker added a feature to ship logs to central log repositories like ELK stack. Most of central logging repositories I know support Graylog Extended Log Format (GELF) and I believe that is <a href="https://docs.docker.com/engine/admin/logging/overview/#configure-the-default-logging-driver">how docker engine ships.</a></p><p>You can also get logs from your infrastructure tools. Most of the popular message brokers (things like kafka, rabbitmq, nsq), HTTP reverse proxies, load balancers, databases, firewalls, application servers, middlewares provide their logs and you can ship them to your central log aggregators.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gtMm0CIE3eNgr4RbSKruDQ.png" /></figure><h3><strong>Metrics</strong></h3><p>Metrics are numbers that aggregatable and measured over intervals of time as time series data. Metrics are optimized for storage, processing of the data since they are just numbers aggregated over intervals of time.</p><p>One of the advantages of metrics based monitoring is overhead of metrics generation and storage is constant, it does not change like logs based monitoring in direct proportion to the increase of the system load. That means Disk or processing utilization does not change based on the increase of traffic. Disk storage utilization only increase based on data on time series database being captured, which happens when you add new metrics’ in your instrumentation in your application code or when you spin up new services/containers/hosts.</p><p>Prometheus (p8s) clients does not send each and every metric to the p8s. Popular prometheus client libraries, for example Coda Hale’s popular <a href="http://metrics.dropwizard.io/">metrics</a> library (and it is java but this project direct ports to different languages) aggregates time series data in your application process and generates metrics output based on in-process calculations. I recommend watching <a href="https://www.youtube.com/watch?v=czes-oa0yik">his presentation on youtube</a> if you want to learn more about his metrics library.</p><p>So, you need to add instrumentation to your application code first if you want to start using Prometheus to collect metrics from your application. You can find a list of <a href="https://prometheus.io/docs/instrumenting/clientlibs/">client libraries</a> on p8s web site. Prometheus works pull based basically you use one of the available to collect metrics in your application and then expose them on your application as accessible via HTTP, generally /metrics endpoint in your application. Then you go an configure prometheus to scrape metrics from your application every few seconds.</p><p>Metrics are far more efficient than querying and aggregating log data. But logs can give you exact data, if you want to get exact average of response times of your server you can log them and then write aggregation queries on elasticsearch. We have to member that metrics are not hundred percent accurate, they relay on some statistical algorithm. Tools like Prometheus and popular metrics client libraries implements some advanced algorithms to give us most accurate numbers. Do not get me wrong! I am not saying use logs, I am saying use both, logs and metrics for the right purpose.</p><p>Finally if you are want to learn Prometheus from strach and if you like learning it from vides like me, I highly recommend this talk: <a href="https://www.youtube.com/watch?v=5GYe_-qqP30">Infrastructure and application monitoring using Prometheus by Marco Pas</a></p><p>Once you collect all your metrics in Prometheus, you can use <a href="http://www.grafana.net">Grafana</a> to visualize those metrics.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hUaJjsAgi0zDH4vuTuL_NA.png" /><figcaption>Metrics on Prometheus</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5314_r4XulcKDKsniFgT1g.png" /><figcaption>Prometheus data visualized on Grafana</figcaption></figure><p><strong>What should I collect?</strong></p><p>Once you have the setup to collect your own this is the question you need to answer. If you are adding metrics for a microservice;</p><p>Firstly, I would recommend you to start with capturing number of requests to observe how busy your service is and how many requests you receive per second/minute. <strong>Number of Requests</strong></p><p>Secondly, I would say start capturing your service’s service time. Basically duration of each request to capture what is your service’s latency. <strong>Service Response Time</strong></p><p>And, I would say capture number of erroneous requests to observe of what percentage of the requests coming to your service are failing. <strong>Error rate of requests.</strong></p><p>Lastly, <strong>always check %95 percentile</strong> if you are not sure what percentile to check. Mean time or average is a happy picture if you want to trick yourself.</p><p>There will be very specific cases for your application, these are just some suggestions you can start to think of. For example in our last project we wanted to measure ETL processing time of each product. We captured each product’s update rate in the underlying system and we calculated the time it took to get to the end of the ETL pipeline. So this way we wanted to see if there is a bottleneck in the Kafka based data streaming pipeline. This way we could observe each stage of data streaming pipeline to identify bottleneck and provision new Kafka Streams containers or Kafka Connect containers when needed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DABlYEqThloWwFpjjJusyg.png" /><figcaption>Monitoring product update latency on our Kafka data streaming pipeline.</figcaption></figure><p>Logs and metrics both needs to exists in your application monitoring stack and they need to be owned by the your team to build, not an IT Ops team. Logs can give you insight about each single request and look for details of what happened exactly at a specific time but metrics can show you context and understand trends in our system.</p><h3><strong>Distributed Tracing</strong></h3><p>When logs can give you insight about a specific time and see what happened it is hard to correlate them when you are building a distributed system. Especially in the era of microservices, a request from a customer can cause hundreds of different service calls in your application.</p><p>Monitoring calls that took longer than expected, call that are failed, why they are failed can be hard to do with logs. Also finding matching logs with a unique request id is something you can accomplish but it would still be hard to query slowest calls that my customers faced.</p><p>Google published a paper with the title <a href="https://research.google.com/pubs/pub36356.html"><strong>Dapper</strong>, a Large-Scale Distributed Systems Tracing Infrastructure</a> at 2010. They talked about how they trace distributed calls. June 2012 Twitter open sourced their internal distributed tracing project, <strong>Zipkin</strong>.</p><p>So if you are in the world of microservices and working on distributed system, you can imagine how valuable to have a visual of correlated distributed calls between services. I tried Zipkin in it’s early years and it was not easy to setup but now in the era of container, it is just one single command. But everyone was not using it and still not using probably. <a href="http://opentracing.io/">OpenTracing</a> was introduced as one single standard as all OSS projects and your application code to instrument your code without depending on one particular tracing vendor.</p><p>So now, you can use one of the <a href="http://opentracing.io/">listed</a> open source client libraries to instrument your code and publish this span information to one of the supported <a href="http://opentracing.io/documentation/pages/supported-tracers.html">tracers</a> (<a href="http://zipkin.io/">Zipkin</a>, <a href="http://uber.github.io/jaeger">Jaeger</a>, Appdash, LightStep, Hawkular, Instane and more).</p><p>If you remember your browsers developer tools and check network tab which calls are being made, it gives you very good insight about what your browsers does, which calls are made in parallell, which ones are taking too long to process and makes your customer wait. Distributed tracers gives you this kind of visualization, on server side.</p><p>For example, you can see which of your services being called, which ones take longer than expected or which ones fail when you receive a request to get list of products under a specific category, order by prices.</p><p>Zipkin interface let’s you query by longest and shortest duration of call stack. So you can focus on your low performing calls and understand which part of the system being a bottleneck. You can also get visualization of dependencies between services which becomes very valuable when you have hundreds of systems talking to each other.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OW_udTV1VSOYflz8kU_q7A.png" /><figcaption>A detailed view of a Zipkin trace</figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8e6cb5434206" width="1" height="1" alt=""><hr><p><a href="https://medium.com/hancengiz/3-pillars-of-observability-8e6cb5434206">3 Pillars of Observability</a> was originally published in <a href="https://medium.com/hancengiz">cengiz han</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]>
</content:encoded>
</item>
<item>
<title>
<![CDATA[ (Micro)services with Event Notifications ]]>
</title>
<link>https://medium.com/hancengiz/micro-services-with-event-notifications-4425ef5be871?source=rss-7a7fc7009fee------2</link>
<guid isPermaLink="false">https://medium.com/p/4425ef5be871</guid>
<category>
<![CDATA[ event-driven-systems ]]>
</category>
<category>
<![CDATA[ microservices ]]>
</category>
<category>
<![CDATA[ event-notification ]]>
</category>
<category>
<![CDATA[ event-sourcing ]]>
</category>
<category>
<![CDATA[ service-oriented ]]>
</category>
<dc:creator>
<![CDATA[ Cengiz Han ]]>
</dc:creator>
<pubDate>Mon, 04 Sep 2017 09:20:51 GMT</pubDate>
<atom:updated>2017-09-05T11:55:03.676Z</atom:updated>
<content:encoded>
<![CDATA[ <p>Microservices is an architectural style that describes software design as independently deployable, loosely coupled services which are modelled around particular business domain.</p><p>Extracting different business domain capabilities from a single process monolith application and creating a system design that contains smaller service processes enables you to <em>scale</em> and <em>deploy</em> each service separately.</p><p>The fact that each service needs to be deployable separately requires and also enables application deployment automation, Continuous Delivery.</p><p>Martin Fowler describes pre requirements of microservices in his articles and emphasises the importance of <a href="https://martinfowler.com/bliki/DevOpsCulture.html">DevOpsCulture</a>.</p><blockquote><a href="https://martinfowler.com/bliki/MicroservicePrerequisites.html">You must be this tall to use microservices</a>.</blockquote><p>I have been mostly working with micro-services and event-driven systems for almost last 7 years. In this post, I will try to explain my view on a particular challenge when designing your service oriented system.</p><p><strong>How they talk to each other?</strong></p><p>One of the fastest way people start with is service to service communication between services. It is a fast way to start making call from to another but it brings lots of problems with it. Going down this route creates a distributed mess and brings problems of all services being available all the time and each services possibly depending on each other which makes things like failing safely really hard, mostly impossible.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*eNEo7vpTnzEQtxuse3fWyw.png" /><figcaption>Microservices directly calling each other. A bit of mess right?</figcaption></figure><p>You always need to plan for failures and how are you going to handle them, how are you going to <strong>fail safely</strong>. Service to service sync communication makes handling failures way much harder. When a business operation needs orchestration of a couple services to be successfully completed you need to <strong><em>start thinking about decoupling services, eventual consistency and bounded context</em></strong> (a way of defining boundaries of a complex domain into business context).</p><blockquote>A system’s being “<strong>fail</strong>-<strong>safe</strong>” means not that <strong>failure</strong> is impossible or improbable, but rather that the system’s design prevents or mitigates unsafe consequences of the system’s <strong>failure</strong>. That is, if and when a “<strong>fail</strong>-<strong>safe</strong>” system “<strong>fails</strong>”, it is “<strong>safe</strong>” or at least no less <strong>safe</strong> than when it was operating correctly. <a href="https://en.wikipedia.org/wiki/Fail-safe">wikipedia</a>.</blockquote><p>For example, what happens if SMTP service is down or you get an error message back from Notification Service when you call it from Order Service? Are you going to cancel the order for that reason? Of course not! If you are doing service to service communication, you need to create a retry mechanism in sync context or persist a state somewhere that needs to be processed by a background running job to keep sending notifications later when notification service is back online and functioning. And you need to think about all the failure scenarios in all service to service communications, no need to say it is a complex solution, <strong><em>guaranteeing reliability and maintainability, and keeping operability</em> <em>simple</em></strong> becomes a hard thing to accomplish.</p><h3>Event Driven Architecture</h3><p>Event Driven Architecture is mostly known as storing all the application state changes as sequence of events. But when you look at the different implementations on different projects we see different usage patterns under the name of Event Sourcing, Command Query Responsibility Segregation (CQRS), Event Notification. Martin Fowler published an article and talked about different type of Event Driven Architecture’s on <em>April, 2017 on a GoTo Conference </em>and created a better classification for different models we were using under the name of Event Driven Architecture. It is not my place to classify them when a guru has done it already.</p><p>I have used CQRS before in two projects, one of them went production, one of them was changed to <strong><em>“Events as secondary concern” </em></strong>as I called back then without knowing the better name to identify the pattern. I now know, it is named as <strong><em>Event Notification.</em></strong></p><blockquote><strong><em>Event notification</em></strong><em>:</em> components communicating via events<br><strong><em>Event-based State Transfer</em></strong><em>:</em> allowing components to access data without calling the source.<br><strong><em>Event Sourcing</em></strong>: using an event log as the primary record for a system<br><strong><em>CQRS</em></strong><em>:</em> having a separate component for updating a store from any readers of the store</blockquote><blockquote>Please see <a href="https://martinfowler.com/articles/201701-event-driven.html">Martin’s post</a> and video on this link for further information about different types.</blockquote><h3>Event Notification</h3><p>There is a slight difference between Event Notification and Event-base State Transfer. In <em>EbST</em> design events contain all the information for consumer service, but Event Notification just contains information about event and consumer services needs to go and get the all context about that event from origin service. Imagine you have an event called OrderCreated and that event contains OrderNumber and maybe some other metadata about order, but if you are creating a Notification Service to send an email/sms to customer about their recent order you might need to go to Order Service and ask details of that order with order number you just received via OrderCreated event.</p><p>Event Notification provides great decoupling and it allows other systems to hook up to events without telling it to source event. You can create a new service which is interested with an event from any service without telling or asking any change to the originating service.</p><p>One gotcha about this design you need to find a way to get all dependencies and which service depends what event from which service. You can not tell what happens in all system when a particular event happens, you can not just read code in source service and see what happens after that event. You need to go through all subscriptions and find out what happens in whole system. This is generally the case we all ignore till we end up in a place we have no idea what happens in the system.</p><p>One of the further things to think about once you started using Event Notification is what goes in to the events? Should you use Event-based State Transfer or do you need to call Order Service to get more information about order that was just created.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*8mj8vDAzgPEfs7lzzgRFAg.png" /><figcaption>Microservices does not know each other, they are decoupled by event notification</figcaption></figure><p>In the above model, when you use Event Notification model you create services that publish events and downstream services subscribes to those events on your event stream. This way, you push the complexity of handling failures in to event stream and event stream processors, your downstream services.</p><p>Order Service publishes an order created event and notification service subscribes to OrderCreated event and send a notification to customer.</p><p>Event store system stores all your events, you can use something like Kafka, RabbitMQ, nsq to store events and dispatch each event type to subscribers. If a service is down temporarily event store sends it to subscriber when they are back, you do not need to implement anything different in this scenario your system design handles it automatically.</p><p>Now you have an event driven architecture, that <em>notifies</em> other system when something happens, when you have a new requirement to react to O<em>rderCreated</em> event you do not need to go and make any changes in your Order Service. All you need to do is create a new subscriber service that subscribes to O<em>rderCreated</em> event.</p><p>Say, you want to show your customers product reviews separately by other customers who actually bought that product. They are verified customer reviews and more valuable to potential buyers, so you create a new subscription in your Customer Product Reviews Service to listen for O<em>rderCreated</em> and other order related events to mark customers as verified customers for that product. Very simple, it does not touch the existing part of the system, you build and deploy it separately without touching upstream service.</p><p>I am planning to write a follow up post with a code example to demonstrate what I mentioned here. Till then I highly recommend to watch <a href="https://martinfowler.com/articles/201701-event-driven.html">Martin’s keynote on goto conference</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4425ef5be871" width="1" height="1" alt=""><hr><p><a href="https://medium.com/hancengiz/micro-services-with-event-notifications-4425ef5be871">(Micro)services with Event Notifications</a> was originally published in <a href="https://medium.com/hancengiz">cengiz han</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]>
</content:encoded>
</item>
<item>
<title>
<![CDATA[ Learn. Make. Share. Repeat. ]]>
</title>
<link>https://medium.com/hancengiz/learn-make-share-repeat-3b60bb60036f?source=rss-7a7fc7009fee------2</link>
<guid isPermaLink="false">https://medium.com/p/3b60bb60036f</guid>
<category>
<![CDATA[ architects ]]>
</category>
<category>
<![CDATA[ agile ]]>
</category>
<category>
<![CDATA[ developer ]]>
</category>
<dc:creator>
<![CDATA[ Cengiz Han ]]>
</dc:creator>
<pubDate>Sun, 03 Sep 2017 17:00:26 GMT</pubDate>
<atom:updated>2017-09-03T17:30:27.913Z</atom:updated>
<content:encoded>
<![CDATA[ <p>It’s been a very long time since I blogged. I was meaning to start again for the last a couple months, here it is I’m kicking it off.</p><p>I joined <a href="https://www.linkedin.com/company/hepsiburada-com">Hepsiburada</a> 10 months ago after working at <a href="https://www.linkedin.com/company/thoughtworks">Thoughtworks</a> as a consultant and a developer for six years. It has been a quite a good journey so far, I am working with passionate individuals and great teams. I am facing different challenges that helps me grow as a leader, architect and at the roots of it as a developer.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/302/1*pNsWcl0HSBwmzVOfPFBh5A.png" /></figure><p>I am currently working with data streaming and search systems heavily and thinking about how to design a system that is going to support business growth plans to serve our customer better by ensuring a fast, scalable and reliable systems.</p><p>I will start blogging about architecture, system design and how to use some open source technology I am in the process of learning, agile teams and leadership. It will definitely not going to be limited to this because my learning journey continues and I will try to reflect it here.</p><p>You can find me on :</p><p><a href="http://github.com/hancengiz">github</a> <a href="http://twitter.com/hancengiz">twitter</a> <a href="https://www.linkedin.com/in/cengizhan/">linkedin</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3b60bb60036f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/hancengiz/learn-make-share-repeat-3b60bb60036f">Learn. Make. Share. Repeat.</a> was originally published in <a href="https://medium.com/hancengiz">cengiz han</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]>
</content:encoded>
</item>
</channel>
</rss>